{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29258e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "import dgl\n",
    "import dgl.data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c1c24d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic dataset with 10000 graphs, ranging from 10 to 500 nodes.\n",
    "dataset = dgl.data.GINDataset(\"PROTEINS\", self_loop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30825b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feature dimensionality: 3\n",
      "Number of graph categories: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Node feature dimensionality:\", dataset.dim_nfeats)\n",
    "print(\"Number of graph categories:\", dataset.gclasses)\n",
    "\n",
    "\n",
    "from dgl.dataloading import GraphDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ce2d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_examples = len(dataset)\n",
    "x_train,x_test = train_test_split(torch.arange(num_examples),test_size=0.2)\n",
    "train_sampler = SubsetRandomSampler(x_train)\n",
    "test_sampler = SubsetRandomSampler(x_test)\n",
    "\n",
    "train_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=train_sampler, batch_size=4, drop_last=False\n",
    ")\n",
    "test_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=test_sampler, batch_size=4, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a66f529b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1113"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f28c3150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Graph(num_nodes=153, num_edges=779,\n",
      "      ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={}), tensor([0, 0, 1, 0])]\n"
     ]
    }
   ],
   "source": [
    "it = iter(test_dataloader)\n",
    "batch = next(it)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a11aa2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes for each graph element in the batch: tensor([24, 17, 32, 80])\n",
      "Number of edges for each graph element in the batch: tensor([130,  81, 160, 408])\n",
      "The original graphs in the minibatch:\n",
      "[Graph(num_nodes=24, num_edges=130,\n",
      "      ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={}), Graph(num_nodes=17, num_edges=81,\n",
      "      ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={}), Graph(num_nodes=32, num_edges=160,\n",
      "      ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={}), Graph(num_nodes=80, num_edges=408,\n",
      "      ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})]\n"
     ]
    }
   ],
   "source": [
    "batched_graph, labels = batch\n",
    "print(\n",
    "    \"Number of nodes for each graph element in the batch:\",\n",
    "    batched_graph.batch_num_nodes(),\n",
    ")\n",
    "print(\n",
    "    \"Number of edges for each graph element in the batch:\",\n",
    "    batched_graph.batch_num_edges(),\n",
    ")\n",
    "\n",
    "# Recover the original graph elements from the minibatch\n",
    "graphs = dgl.unbatch(batched_graph)\n",
    "print(\"The original graphs in the minibatch:\")\n",
    "print(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b046f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        g.ndata[\"h\"] = h\n",
    "        return dgl.mean_nodes(g, \"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1b390b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch.conv import GINConv\n",
    "from dgl.nn.pytorch.glob import SumPooling\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Construct two-layer MLP-type aggreator for GIN model\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList()\n",
    "        # two-layer MLP\n",
    "        self.linears.append(nn.Linear(input_dim, hidden_dim, bias=False))\n",
    "        self.linears.append(nn.Linear(hidden_dim, output_dim, bias=False))\n",
    "        self.batch_norm = nn.BatchNorm1d((hidden_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        h = F.relu(self.batch_norm(self.linears[0](h)))\n",
    "        return self.linears[1](h)\n",
    "\n",
    "\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.ginlayers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        num_layers = 5\n",
    "        # five-layer GCN with two-layer MLP aggregator and sum-neighbor-pooling scheme\n",
    "        for layer in range(num_layers - 1):  # excluding the input layer\n",
    "            if layer == 0:\n",
    "                mlp = MLP(input_dim, hidden_dim, hidden_dim)\n",
    "            else:\n",
    "                mlp = MLP(hidden_dim, hidden_dim, hidden_dim)\n",
    "            self.ginlayers.append(\n",
    "                GINConv(mlp, learn_eps=False)\n",
    "            )  # set to True if learning epsilon\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        # linear functions for graph sum poolings of output of each layer\n",
    "        self.linear_prediction = nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            if layer == 0:\n",
    "                self.linear_prediction.append(nn.Linear(input_dim, output_dim))\n",
    "            else:\n",
    "                self.linear_prediction.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.pool = (\n",
    "            SumPooling()\n",
    "        )  # change to mean readout (AvgPooling) on social network datasets\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # list of hidden representation at each layer (including the input layer)\n",
    "        hidden_rep = [h]\n",
    "        for i, layer in enumerate(self.ginlayers):\n",
    "            h = layer(g, h)\n",
    "            h = self.batch_norms[i](h)\n",
    "            h = F.relu(h)\n",
    "            hidden_rep.append(h)\n",
    "        score_over_layer = 0\n",
    "        # perform graph sum pooling over all nodes in each layer\n",
    "        for i, h in enumerate(hidden_rep):\n",
    "            pooled_h = self.pool(g, h)\n",
    "            score_over_layer += self.drop(self.linear_prediction[i](pooled_h))\n",
    "        return score_over_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e5792",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0Test accuracy: 0.5605381165919282\n",
      "1Test accuracy: 0.6547085201793722\n",
      "2Test accuracy: 0.6681614349775785\n",
      "3Test accuracy: 0.6098654708520179\n",
      "4Test accuracy: 0.547085201793722\n",
      "5Test accuracy: 0.6591928251121076\n",
      "6Test accuracy: 0.6053811659192825\n",
      "7Test accuracy: 0.5695067264573991\n",
      "8Test accuracy: 0.6502242152466368\n",
      "9Test accuracy: 0.6995515695067265\n",
      "10Test accuracy: 0.6905829596412556\n",
      "11Test accuracy: 0.672645739910314\n",
      "12Test accuracy: 0.6457399103139013\n",
      "13Test accuracy: 0.6278026905829597\n",
      "14Test accuracy: 0.695067264573991\n",
      "15Test accuracy: 0.6636771300448431\n",
      "16Test accuracy: 0.6502242152466368\n",
      "17Test accuracy: 0.6188340807174888\n",
      "18Test accuracy: 0.600896860986547\n",
      "19Test accuracy: 0.5739910313901345\n",
      "20Test accuracy: 0.6412556053811659\n",
      "21Test accuracy: 0.6233183856502242\n",
      "22Test accuracy: 0.6322869955156951\n",
      "23Test accuracy: 0.5112107623318386\n",
      "24Test accuracy: 0.695067264573991\n",
      "25Test accuracy: 0.6412556053811659\n",
      "26Test accuracy: 0.6591928251121076\n",
      "27Test accuracy: 0.6502242152466368\n",
      "28Test accuracy: 0.6502242152466368\n",
      "29Test accuracy: 0.672645739910314\n",
      "30Test accuracy: 0.6681614349775785\n",
      "31Test accuracy: 0.7130044843049327\n",
      "32Test accuracy: 0.6995515695067265\n",
      "33Test accuracy: 0.6322869955156951\n",
      "34Test accuracy: 0.5426008968609866\n",
      "35Test accuracy: 0.4977578475336323\n",
      "36Test accuracy: 0.7130044843049327\n"
     ]
    }
   ],
   "source": [
    "# Create the model with given dimensions\n",
    "model = GIN(dataset.dim_nfeats, 16, dataset.gclasses).to('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for batched_graph, labels in train_dataloader:\n",
    "#         print(labels)\n",
    "        batched_graph, labels = batched_graph.to('cuda'), labels.to('cuda')\n",
    "        pred = model(batched_graph, batched_graph.ndata[\"attr\"].float())\n",
    "        loss = F.cross_entropy(pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    num_correct = 0\n",
    "    num_tests = 0\n",
    "    for batched_graph, labels in test_dataloader:\n",
    "#         print(labels)\n",
    "        batched_graph, labels = batched_graph.to('cuda'), labels.to('cuda')\n",
    "        pred = model(batched_graph, batched_graph.ndata[\"attr\"].float())\n",
    "        num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "        num_tests += len(labels)\n",
    "\n",
    "    print(f\"{epoch}Test accuracy:\", num_correct / num_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f58073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl",
   "language": "python",
   "name": "dgl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
