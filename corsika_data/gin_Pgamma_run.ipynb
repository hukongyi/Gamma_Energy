{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cd82d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import sparse\n",
    "import os\n",
    "from sklearn import metrics\n",
    "\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "import dgl\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ef2b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f790b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDataset_dgl(DGLDataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.label = np.load(os.path.join(self.path, f\"label.npy\"))\n",
    "        self.label = torch.tensor(self.label, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        g, _ = dgl.load_graphs(os.path.join(self.path, f\"graph.bin\"), [index])\n",
    "        return (\n",
    "            g[0],\n",
    "            self.label[index],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c59f9a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCdataset_train = MCDataset_dgl(\"/tmp/hky/gnndata_gin_Pgamma/train_data/\")\n",
    "MCdataset_val = MCDataset_dgl(\"/tmp/hky/gnndata_gin_Pgamma/val_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eceae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = GraphDataLoader(MCdataset_train, batch_size=64, drop_last=False,num_workers=8,shuffle=True)\n",
    "val_dataloader = GraphDataLoader(MCdataset_val, batch_size=64, drop_last=False,num_workers=8,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23613641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch.conv import GINConv\n",
    "from dgl.nn.pytorch.glob import SumPooling, AvgPooling, MaxPooling\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Construct two-layer MLP-type aggreator for GIN model\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList()\n",
    "        # two-layer MLP\n",
    "        self.linears.append(nn.Linear(input_dim, hidden_dim, bias=False))\n",
    "        self.linears.append(nn.Linear(hidden_dim, output_dim, bias=False))\n",
    "        self.batch_norm = nn.BatchNorm1d((hidden_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        h = F.relu(self.batch_norm(self.linears[0](h)))\n",
    "        return self.linears[1](h)\n",
    "\n",
    "\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.ginlayers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        num_layers = 5\n",
    "        # five-layer GCN with two-layer MLP aggregator and sum-neighbor-pooling scheme\n",
    "        for layer in range(num_layers - 1):  # excluding the input layer\n",
    "            if layer == 0:\n",
    "                mlp = MLP(input_dim, hidden_dim, hidden_dim)\n",
    "            else:\n",
    "                mlp = MLP(hidden_dim, hidden_dim, hidden_dim)\n",
    "            self.ginlayers.append(\n",
    "                GINConv(mlp, learn_eps=False)\n",
    "            )  # set to True if learning epsilon\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        # linear functions for graph sum poolings of output of each layer\n",
    "        self.linear_prediction = nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            if layer == 0:\n",
    "                self.linear_prediction.append(nn.Linear(input_dim, output_dim))\n",
    "            else:\n",
    "                self.linear_prediction.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.pool = (\n",
    "            MaxPooling()\n",
    "        )  # change to mean readout (AvgPooling) on social network datasets\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # list of hidden representation at each layer (including the input layer)\n",
    "        hidden_rep = [h]\n",
    "        for i, layer in enumerate(self.ginlayers):\n",
    "            h = layer(g, h)\n",
    "            h = self.batch_norms[i](h)\n",
    "            h = F.relu(h)\n",
    "            hidden_rep.append(h)\n",
    "        score_over_layer = 0\n",
    "        # perform graph sum pooling over all nodes in each layer\n",
    "        for i, h in enumerate(hidden_rep):\n",
    "            pooled_h = self.pool(g, h)\n",
    "            score_over_layer += self.drop(self.linear_prediction[i](pooled_h))\n",
    "        return score_over_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7204bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=83, num_edges=6806,\n",
       "      ndata_schemes={'xdata': Scheme(shape=(9,), dtype=torch.float32)}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCdataset_train[0][0].to(device)\n",
    "# model = GIN(9, 64, 2).to(device)\n",
    "# for batched_graph, labels in train_dataloader:\n",
    "#     print(batched_graph.num_nodes())\n",
    "# print(labels)\n",
    "# batched_graph, labels = batched_graph.to(device), labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "733889aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/hky/miniconda3/envs/dgl/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.5017\n",
      "auc:0.8922\n",
      "1\n",
      "loss:0.3481\n",
      "auc:0.9237\n",
      "2\n",
      "loss:0.3412\n",
      "auc:0.9200\n",
      "3\n",
      "loss:0.3300\n",
      "auc:0.8973\n",
      "4\n",
      "loss:0.3323\n",
      "auc:0.9256\n",
      "5\n",
      "loss:0.3172\n",
      "auc:0.9270\n",
      "6\n",
      "loss:0.3140\n",
      "auc:0.9036\n",
      "7\n",
      "loss:0.3057\n",
      "auc:0.9326\n",
      "8\n",
      "loss:0.3069\n",
      "auc:0.9385\n",
      "9\n",
      "loss:0.3030\n",
      "auc:0.9220\n",
      "10\n",
      "loss:0.3022\n",
      "auc:0.9198\n",
      "11\n",
      "loss:0.2936\n",
      "auc:0.9073\n",
      "12\n",
      "loss:0.2952\n",
      "auc:0.9428\n",
      "13\n",
      "loss:0.2930\n",
      "auc:0.9332\n",
      "14\n",
      "loss:0.3027\n",
      "auc:0.9319\n",
      "15\n",
      "loss:0.2929\n",
      "auc:0.9267\n",
      "16\n",
      "loss:0.2858\n",
      "auc:0.9371\n",
      "17\n",
      "loss:0.2858\n",
      "auc:0.9097\n",
      "18\n",
      "loss:0.2801\n",
      "auc:0.9278\n",
      "19\n",
      "loss:0.2828\n",
      "auc:0.9402\n",
      "20\n",
      "loss:0.2818\n",
      "auc:0.9437\n",
      "21\n",
      "loss:0.2746\n",
      "auc:0.9332\n",
      "22\n",
      "loss:0.2738\n",
      "auc:0.9172\n",
      "23\n",
      "loss:0.2774\n",
      "auc:0.9146\n",
      "24\n",
      "loss:0.2736\n",
      "auc:0.9279\n",
      "25\n",
      "loss:0.2730\n",
      "auc:0.9127\n",
      "26\n",
      "loss:0.2687\n",
      "auc:0.9240\n",
      "27\n",
      "loss:0.2711\n",
      "auc:0.9173\n",
      "28\n",
      "loss:0.2677\n",
      "auc:0.9405\n",
      "29\n",
      "loss:0.2664\n",
      "auc:0.9195\n",
      "30\n",
      "loss:0.2687\n",
      "auc:0.9387\n",
      "31\n",
      "loss:0.2648\n",
      "auc:0.9323\n",
      "32\n",
      "loss:0.2662\n",
      "auc:0.9383\n",
      "33\n",
      "loss:0.2657\n",
      "auc:0.9102\n",
      "34\n",
      "loss:0.2661\n",
      "auc:0.9298\n",
      "35\n",
      "loss:0.2657\n",
      "auc:0.9162\n",
      "36\n",
      "loss:0.2645\n",
      "auc:0.9261\n",
      "37\n",
      "loss:0.2617\n",
      "auc:0.9254\n",
      "38\n",
      "loss:0.2636\n",
      "auc:0.9404\n",
      "39\n",
      "loss:0.2613\n",
      "auc:0.9367\n",
      "40\n",
      "loss:0.2566\n",
      "auc:0.9268\n",
      "41\n",
      "loss:0.2617\n",
      "auc:0.9295\n",
      "42\n",
      "loss:0.2571\n",
      "auc:0.9362\n",
      "43\n",
      "loss:0.2571\n",
      "auc:0.9255\n",
      "44\n",
      "loss:0.2573\n",
      "auc:0.9238\n",
      "45\n",
      "loss:0.2536\n",
      "auc:0.9241\n",
      "46\n",
      "loss:0.2494\n",
      "auc:0.9305\n",
      "47\n",
      "loss:0.2483\n",
      "auc:0.9136\n",
      "48\n",
      "loss:0.2492\n",
      "auc:0.9259\n",
      "49\n",
      "loss:0.2495\n",
      "auc:0.9254\n",
      "50\n",
      "loss:0.2500\n",
      "auc:0.9294\n",
      "51\n",
      "loss:0.2458\n",
      "auc:0.9343\n",
      "52\n",
      "loss:0.2471\n",
      "auc:0.9296\n",
      "53\n",
      "loss:0.2473\n",
      "auc:0.9267\n",
      "54\n",
      "loss:0.2480\n",
      "auc:0.9300\n",
      "55\n",
      "loss:0.2422\n",
      "auc:0.9385\n",
      "56\n",
      "loss:0.2448\n",
      "auc:0.9298\n",
      "57\n",
      "loss:0.2515\n",
      "auc:0.9352\n",
      "58\n",
      "loss:0.2419\n",
      "auc:0.9393\n",
      "59\n",
      "loss:0.2415\n",
      "auc:0.9280\n",
      "60\n",
      "loss:0.2402\n",
      "auc:0.9443\n",
      "61\n",
      "loss:0.2451\n",
      "auc:0.9367\n",
      "62\n",
      "loss:0.2417\n",
      "auc:0.9187\n",
      "63\n",
      "loss:0.2384\n",
      "auc:0.9380\n",
      "64\n",
      "loss:0.2393\n",
      "auc:0.9456\n",
      "65\n",
      "loss:0.2387\n",
      "auc:0.9382\n",
      "66\n",
      "loss:0.2369\n",
      "auc:0.9145\n",
      "67\n",
      "loss:0.2388\n",
      "auc:0.9179\n",
      "68\n",
      "loss:0.2358\n",
      "auc:0.9391\n",
      "69\n",
      "loss:0.2369\n",
      "auc:0.9303\n",
      "70\n",
      "loss:0.2353\n",
      "auc:0.9131\n",
      "71\n",
      "loss:0.2335\n",
      "auc:0.9171\n",
      "72\n",
      "loss:0.2359\n",
      "auc:0.9208\n",
      "73\n",
      "loss:0.2325\n",
      "auc:0.9290\n",
      "74\n",
      "loss:0.2320\n",
      "auc:0.9318\n",
      "75\n",
      "loss:0.2346\n",
      "auc:0.9141\n",
      "76\n",
      "loss:0.2305\n",
      "auc:0.9225\n",
      "77\n",
      "loss:0.2304\n",
      "auc:0.9192\n",
      "78\n",
      "loss:0.2329\n",
      "auc:0.9178\n",
      "79\n",
      "loss:0.2329\n",
      "auc:0.9075\n",
      "80\n",
      "loss:0.2330\n",
      "auc:0.9334\n",
      "81\n",
      "loss:0.2294\n",
      "auc:0.9333\n",
      "82\n",
      "loss:0.2333\n",
      "auc:0.9265\n",
      "83\n",
      "loss:0.2309\n",
      "auc:0.9197\n",
      "84\n",
      "loss:0.2357\n",
      "auc:0.9182\n",
      "85\n",
      "loss:0.2300\n",
      "auc:0.9264\n",
      "86\n",
      "loss:0.2294\n",
      "auc:0.9301\n",
      "87\n",
      "loss:0.2300\n",
      "auc:0.9182\n",
      "88\n",
      "loss:0.2335\n",
      "auc:0.9313\n",
      "89\n",
      "loss:0.2293\n",
      "auc:0.9295\n",
      "90\n",
      "loss:0.2303\n",
      "auc:0.9380\n",
      "91\n",
      "loss:0.2295\n",
      "auc:0.9294\n",
      "92\n",
      "loss:0.2287\n",
      "auc:0.9361\n",
      "93\n",
      "loss:0.2343\n",
      "auc:0.9234\n",
      "94\n",
      "loss:0.2277\n",
      "auc:0.9266\n",
      "95\n",
      "loss:0.2282\n",
      "auc:0.9140\n",
      "96\n",
      "loss:0.2329\n",
      "auc:0.9293\n",
      "97\n",
      "loss:0.2296\n",
      "auc:0.9071\n",
      "98\n",
      "loss:0.2280\n",
      "auc:0.9201\n",
      "99\n",
      "loss:0.2291\n",
      "auc:0.9292\n"
     ]
    }
   ],
   "source": [
    "model = GIN(9, 64, 2).to(device)\n",
    "maxtpoch = 100\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, maxtpoch\n",
    ")\n",
    "m = nn.Softmax(dim=1)\n",
    "\n",
    "for epoch in range(maxtpoch):\n",
    "    sumloss = list()\n",
    "    print(epoch)\n",
    "    model.train()\n",
    "    for batched_graph, labels in train_dataloader:\n",
    "        if(batched_graph.num_edges()>2e8):\n",
    "                continue\n",
    "        batched_graph, labels = batched_graph.to(device), labels.to(device)\n",
    "        pred = model(batched_graph, batched_graph.ndata[\"xdata\"])\n",
    "        loss = F.cross_entropy(pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        sumloss.append(loss.item())\n",
    "        optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    print(f\"loss:{np.mean(sumloss):.4f}\")\n",
    "    y_pred = list()\n",
    "    y_orgin = list()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batched_graph, labels in val_dataloader:\n",
    "            if(batched_graph.num_edges()>2e8):\n",
    "                continue\n",
    "            batched_graph, labels = batched_graph.to(device), labels.to(device)\n",
    "            pred = model(batched_graph, batched_graph.ndata[\"xdata\"])\n",
    "           \n",
    "            y_pred.append(pred[:, 1].cpu().numpy())\n",
    "            y_orgin.append(labels.cpu().numpy())\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_orgin = np.concatenate(y_orgin)\n",
    "    auc = metrics.roc_auc_score(y_orgin, y_pred)\n",
    "    print(f\"auc:{auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2a6e71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl",
   "language": "python",
   "name": "dgl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
